{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords'ları çıkarmak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reddit_comments.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Yorumları yükle\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m reddit_comments \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreddit_comments.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Türkçe stopwords listesi ve kendi stopwords listenizi ekleyin\u001b[39;00m\n\u001b[0;32m     13\u001b[0m turkish_stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macaba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mama\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mancak\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martık\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maslında\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbana\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbazen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbazı\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbelki\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mben\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenden\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeni\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mberi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiraz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbirçok\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbirşey\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbizden\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbizi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbizim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mböyle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mböylece\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuna\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myetmiş\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myirmi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoksa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myüz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzaten\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzira\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mküfür1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mküfür2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mküfür3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mküfür4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mküfür5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\info\\OneDrive\\Masaüstü\\anket\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\info\\OneDrive\\Masaüstü\\anket\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\info\\OneDrive\\Masaüstü\\anket\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\info\\OneDrive\\Masaüstü\\anket\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\info\\OneDrive\\Masaüstü\\anket\\.venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_comments.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Yorumları yükle\n",
    "reddit_comments = pd.read_csv(\"ekonomi_birlesim.csv\")\n",
    "\n",
    "# Türkçe stopwords listesi ve kendi stopwords listenizi ekleyin\n",
    "turkish_stopwords = set([\n",
    "    \"acaba\", \"ama\", \"ancak\", \"artık\", \"aslında\", \"az\", \"bana\", \"bazen\", \"bazı\", \"belki\", \"ben\", \"benden\", \"beni\", \"benim\", \n",
    "    \"beri\", \"bir\", \"biraz\", \"birçok\", \"birşey\", \"biz\", \"bize\", \"bizden\", \"bizi\", \"bizim\", \"böyle\", \"böylece\", \"bu\", \"buna\", \n",
    "    \"bunda\", \"bundan\", \"bunu\", \"bunun\", \"burada\", \"bütün\", \"çoğu\", \"çoğunu\", \"çok\", \"çünkü\", \"da\", \"daha\", \"dahi\", \"de\", \n",
    "    \"defa\", \"diye\", \"dolayı\", \"elbette\", \"en\", \"fakat\", \"falan\", \"filan\", \"gibi\", \"gene\", \"gereği\", \"göre\", \"hala\", \"hangi\", \n",
    "    \"hani\", \"hatta\", \"hem\", \"henüz\", \"hep\", \"hepsi\", \"her\", \"herhangi\", \"herkes\", \"herkese\", \"herkesi\", \"herkesin\", \"hiç\", \n",
    "    \"hiçbir\", \"için\", \"iki\", \"ile\", \"ilgili\", \"ise\", \"işte\", \"itibaren\", \"kaç\", \"kadar\", \"karşın\", \"kendi\", \"kendine\", \n",
    "    \"kendini\", \"kendisi\", \"kez\", \"ki\", \"kim\", \"kime\", \"kimi\", \"kimin\", \"kimisi\", \"kimse\", \"kısaca\", \"kadar\", \"mı\", \"mi\", \n",
    "    \"mu\", \"mü\", \"nasıl\", \"ne\", \"neden\", \"nedenle\", \"nerde\", \"nerede\", \"nereye\", \"neyse\", \"niçin\", \"niye\", \"o\", \"öyle\", \n",
    "    \"oldu\", \"olduğu\", \"olduğunu\", \"olarak\", \"olduktan\", \"olmadı\", \"olmadığı\", \"olmak\", \"olması\", \"olmayan\", \"olmaz\", \n",
    "    \"olsa\", \"olsun\", \"olup\", \"olur\", \"olursa\", \"oluyor\", \"onlar\", \"onlardan\", \"onları\", \"onların\", \"onu\", \"onun\", \"orada\", \n",
    "    \"öte\", \"ötürü\", \"öyle\", \"pek\", \"rağmen\", \"sadece\", \"sanki\", \"şayet\", \"şekilde\", \"şimdi\", \"siz\", \"sizden\", \"sizi\", \n",
    "    \"sizin\", \"sonra\", \"şöyle\", \"şu\", \"şuna\", \"şunda\", \"şundan\", \"şunlar\", \"şunu\", \"şunun\", \"ta\", \"tabii\", \"tam\", \"tamam\", \n",
    "    \"tamamen\", \"tarafından\", \"trilyon\", \"tüm\", \"üç\", \"üzere\", \"var\", \"vardı\", \"ve\", \"veya\", \"veyahut\", \"ya\", \"yani\", \n",
    "    \"yapacak\", \"yapılan\", \"yapılması\", \"yapıyor\", \"yapmak\", \"yaptı\", \"yaptığı\", \"yaptığını\", \"yaptıkları\", \"yedi\", \"yerine\", \n",
    "    \"yetmiş\", \"yine\", \"yirmi\", \"yoksa\", \"yüz\", \"zaten\", \"zira\", \"küfür1\", \"küfür2\", \"küfür3\", \"küfür4\", \"küfür5\"\n",
    "])\n",
    "\n",
    "# Metin ön işleme\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Küçük harfe çevir\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Noktalama işaretlerini kaldır\n",
    "    text = ' '.join([word for word in text.split() if word not in turkish_stopwords])  # Stop kelimeleri kaldır\n",
    "    return text\n",
    "\n",
    "reddit_comments['Processed_Comment'] = reddit_comments['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Özellik çıkarma (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(reddit_comments['Processed_Comment'])\n",
    "\n",
    "# K-means algoritması ile kümeleme\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "reddit_comments['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Kullanıcı verilerini yükle\n",
    "user_data = pd.read_csv(\"realistic_user_data.csv\")\n",
    "\n",
    "# Kullanıcıları yorum kümelerine göre atama\n",
    "user_data['Comment'] = reddit_comments['Comment'].sample(n=len(user_data), replace=True).values\n",
    "user_data['Comment_Cluster'] = reddit_comments['Cluster'].sample(n=len(user_data), replace=True).values\n",
    "\n",
    "# Kümeleme sonuçlarını incele\n",
    "print(user_data[['Age', 'Job', 'Depression', 'Comment', 'Comment_Cluster', 'Anxiety', 'Obesity', 'Active']].head(20))\n",
    "\n",
    "# Veriyi CSV olarak kaydet\n",
    "user_data.to_csv(\"clustered_user_data_with_comments.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Kullanıcı verileri yorum kümeleri ile eşleştirildi ve kaydedildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Önce küçük/büyük harf ve boşlukların kontrolünü sağlama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Optimized text preprocessing function with compiled regex patterns\n",
    "    and more efficient operations.\n",
    "    \"\"\"\n",
    "    # Compile regex patterns once (more efficient)\n",
    "    punctuation_pattern = re.compile(r\"[,.:;!?()\\\"\\'\\[\\]]\")\n",
    "    newline_pattern = re.compile(r\"\\n+\")\n",
    "    whitespace_pattern = re.compile(r\"\\s+\")\n",
    "    \n",
    "    # Chain operations efficiently\n",
    "    text = (text.lower()  # Convert to lowercase\n",
    "           .strip())      # Remove leading/trailing whitespace\n",
    "    \n",
    "    # Apply compiled patterns\n",
    "    text = punctuation_pattern.sub(\"\", text)\n",
    "    text = newline_pattern.sub(\" \", text)\n",
    "    text = whitespace_pattern.sub(\" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# File handling with context manager\n",
    "def process_file(input_file, output_file):\n",
    "    try:\n",
    "        # Read the input file\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Process the text column (assuming it exists)\n",
    "        if 'Body' in df.columns:\n",
    "            df['cleaned_text'] = df['Body'].apply(preprocess_text)\n",
    "        \n",
    "        # Save to output file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "input_file = \"C:\\\\Users\\\\info\\\\OneDrive\\\\Masaüstü\\\\anket\\\\yorumlar\\\\ekonomi_birlesim.csv\"\n",
    "output_file = \"ekonomi_cleaned2.csv\"\n",
    "\n",
    "if process_file(input_file, output_file):\n",
    "    print(\"Text preprocessing completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metin sütunu işleniyor...\n",
      "Metin temizleme işlemi tamamlandı! Toplam 1538 satır işlendi.\n",
      "Temizlenmiş metinler 'C:\\Users\\info\\OneDrive\\Masaüstü\\anket\\ekonomi2\\ekonomi_cleaned.csv' dosyasına başarıyla kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Optimize edilmiş metin temizleme fonksiyonu: \n",
    "    - Noktalama işaretleri, fazla boşluklar ve yeni satır karakterleri temizlenir.\n",
    "    \"\"\"\n",
    "    # NaN veya geçersiz metinleri kontrol et\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Regex desenleri (önceden derlenmiş)\n",
    "    punctuation_pattern = re.compile(r\"[,.:;!?()\\\"\\'\\[\\]]\")  # Noktalama işaretlerini kaldır\n",
    "    newline_pattern = re.compile(r\"\\n+\")                    # Yeni satır karakterlerini kaldır\n",
    "    whitespace_pattern = re.compile(r\"\\s+\")                 # Fazla boşlukları tek boşlukla değiştir\n",
    "    \n",
    "    # Metni temizleme işlemleri\n",
    "    text = text.lower().strip()                # Küçük harf ve baştaki/sondaki boşluklar\n",
    "    text = punctuation_pattern.sub(\"\", text)   # Noktalama kaldır\n",
    "    text = newline_pattern.sub(\" \", text)      # Yeni satırları boşlukla değiştir\n",
    "    text = whitespace_pattern.sub(\" \", text)   # Fazla boşlukları tek boşlukla değiştir\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Giriş dosyasındaki 'Body' sütununu temizler ve 'Metin' adında yeni sütun ekler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # CSV dosyasını yükle\n",
    "        df = pd.read_csv(input_file, encoding='utf-8')\n",
    "        \n",
    "        # Giriş dosyasında 'Body' sütunu kontrolü\n",
    "        if 'Body' in df.columns:\n",
    "            print(\"Metin sütunu işleniyor...\")\n",
    "            \n",
    "            # Metin temizleme fonksiyonunu uygula ve 'Metin' adlı yeni sütuna yaz\n",
    "            df['Metin'] = df['Body'].astype(str).apply(preprocess_text)\n",
    "            \n",
    "            # Sonuç dosyasını kaydet\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            print(f\"Metin temizleme işlemi tamamlandı! Toplam {len(df)} satır işlendi.\")\n",
    "            return True\n",
    "        else:\n",
    "            raise ValueError(\"Giriş dosyasında 'Body' adlı sütun bulunamadı!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Kullanım\n",
    "input_file = \"C:\\\\Users\\\\info\\\\OneDrive\\\\Masaüstü\\\\anket\\\\ekonomi2\\\\ekonomi_birlesim.csv\"\n",
    "output_file = \"C:\\\\Users\\\\info\\\\OneDrive\\\\Masaüstü\\\\anket\\\\ekonomi2\\\\ekonomi_cleaned.csv\"\n",
    "\n",
    "if process_file(input_file, output_file):\n",
    "    print(f\"Temizlenmiş metinler '{output_file}' dosyasına başarıyla kaydedildi.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwordsleri çıkarma işlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Yorumları yükle\n",
    "reddit_comments = pd.read_csv(\"reddit_comments.csv\")\n",
    "\n",
    "# Metin ön işleme\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Küçük harfe çevir\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Noktalama işaretlerini kaldır\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Stop kelimeleri kaldır\n",
    "    return text\n",
    "\n",
    "reddit_comments['Processed_Comment'] = reddit_comments['Comment'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Özellik çıkarma (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(reddit_comments['Processed_Comment'])\n",
    "\n",
    "# K-means algoritması ile kümeleme\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "reddit_comments['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Kullanıcı verilerini yükle\n",
    "user_data = pd.read_csv(\"realistic_user_data.csv\")\n",
    "\n",
    "# Kullanıcıları yorum kümelerine göre atama\n",
    "user_data['Comment'] = reddit_comments['Comment'].sample(n=len(user_data), replace=True).values\n",
    "user_data['Comment_Cluster'] = reddit_comments['Cluster'].sample(n=len(user_data), replace=True).values\n",
    "\n",
    "# Kümeleme sonuçlarını incele\n",
    "print(user_data[['Age', 'Job', 'Depression', 'Comment', 'Comment_Cluster', 'Anxiety', 'Obesity', 'Active']].head(20))\n",
    "\n",
    "# Veriyi CSV olarak kaydet\n",
    "user_data.to_csv(\"clustered_user_data_with_comments.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Kullanıcı verileri yorum kümeleri ile eşleştirildi ve kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
